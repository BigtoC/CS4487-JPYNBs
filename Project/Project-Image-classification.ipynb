{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5RtmpwqNyvvw"
   },
   "source": [
    "# CS4487 Project: Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Reproduction\n",
    "* ### Environment Requirments\n",
    "  * #### Python Version >= 3.7\n",
    "  * #### Pytorch Version =1.3\n",
    "  * #### Nvdia GPU (for speed up training and testing)\n",
    "* ### Run Codes\n",
    "  * #### Run all the code from top to bottom\n",
    "  * #### The first code cell helps install a progress bar library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CM4u04Ptyvvz"
   },
   "source": [
    "## Regarding course project\n",
    "* External training data is not allowed. Learning can only be performed on the 50,000 training images in **CIFAR10**\n",
    "* The [**CIFAR10** dataset](https://www.cs.toronto.edu/~kriz/cifar.html) was comprised of 60,000 32 Ã— 32 color photographs of objects from 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images.\n",
    "* Submit the results at [Kaggle CS4487 Course Project](https://www.kaggle.com/c/cs4487cp/)\n",
    "* The second test set containing roughly 2,000 images (refer to test_data/y_test.npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KC9dhUb2yvvz"
   },
   "source": [
    "## Evaluation Criteria \n",
    "\n",
    "* ### Classification Accuaracy:\n",
    "  ### $$ Acc(f, D) = \\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{I}[y^i = f(x^i)]$$\n",
    "\n",
    "* ### File Format:\n",
    "Upload a file in CSV format, such as:\n",
    "\n",
    "|         **Index**         |         **Category**        |\n",
    "|---------------------------|-----------------------------|\n",
    "| 0                         | 8                           |\n",
    "| 1                         | 4                           |\n",
    "| 2                         | 6                           |\n",
    "| 3                         | 1                           |\n",
    "| ......                    | ......                      |\n",
    "| 11999                     | 2                           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlAURl9gyvv1"
   },
   "source": [
    "PyTorch Tutorial: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hEdlTSHSyvv3"
   },
   "source": [
    "---\n",
    "## Table of content (for quick jump)\n",
    "* ### [Import Library](#Import-Library)\n",
    "* ### [Data Preparation](#Data-Preparation)\n",
    "* ### [Common Functions](#Common-Functions)\n",
    "  * #### [Training Function](#Training-function)\n",
    "  * #### [Testing Function](#Testing-function)\n",
    "  * #### [Test for each category](#Test-for-each-category)\n",
    "  * #### [Save Model](#Save-Model)\n",
    "  * #### [Print Time](#Print-Time)\n",
    "  * #### [Progress Bar](#Progress-Bar)\n",
    "* ### [DenseNet (DN)](#DenseNet)\n",
    "  * #### [Training](#Train-DenaseNet)\n",
    "  * #### [Testing](#Test-DenseNet)\n",
    "* ### [Run the model](#Run-the-model-in-project-test-data)\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gm8PdbZpyvv4"
   },
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install progressbar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5IH1vwTyvv5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "from _datetime import datetime\n",
    "import pytz\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from progressbar import *\n",
    "from sys import platform\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_root = \".\"  # For loacal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1A24rM-yvv8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "basic_path = \"basic_cifar_net.pth\"\n",
    "PATH_DPN = \"DPN_cifar.pth\"\n",
    "PATH_Dense = \"DenseNet.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RBvQrtukyvwA"
   },
   "source": [
    "---\n",
    "---\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8Ar3LQxyvwB"
   },
   "source": [
    "### Read CIFAR10 data\n",
    "* The archive contains the files data_batch_1, data_batch_2, ..., data_batch_5, as well as test_batch. Each of these files is a Python \"pickled\" object produced with cPickle.\n",
    "* Loaded in this way, each of the batch files contains a dictionary with the following elements:\n",
    "  * **data**: \n",
    "    * a 10,000 x 3,072 numpy array of uint8s. Each row of the array stores a 32 x 32 colour image. \n",
    "    * The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. \n",
    "    * The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
    "  * **labels**: \n",
    "    * a list of 10,000 numbers in the range 0 - 9. \n",
    "    * The number at index i indicates the label of the ith image in the array data.\n",
    "* The dataset contains another file, called batches.meta. It too contains a Python dictionary object. It has the following entries:\n",
    "  * **label_names** -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. \n",
    "  * For example, label_names[0] == \"airplane\", label_names[1] == \"automobile\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37XbAGx7yvwB"
   },
   "source": [
    "### Loading and normalizing CIFAR10 to dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oyQkgqNOyvwC"
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DB2gBIfPyvwE"
   },
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root=f'{gd_root}/data', train=True, download=False, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, \n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True, \n",
    "                                           num_workers=8, \n",
    "                                           pin_memory=True\n",
    "                                          )\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=f'{gd_root}/data', train=False, download=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, \n",
    "                                          batch_size=32,\n",
    "                                          shuffle=False, \n",
    "                                          num_workers=8,\n",
    "                                          pin_memory=True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3289,
     "status": "ok",
     "timestamp": 1576304876813,
     "user": {
      "displayName": "Tai To CHAN",
      "photoUrl": "",
      "userId": "16856483288969111373"
     },
     "user_tz": -480
    },
    "id": "iZX-q8HpyvwH",
    "outputId": "8344fa58-1354-40ea-f53b-b4c8217ca5eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(trainset)\n",
    "train_size\n",
    "test_size = len(testset)\n",
    "test_size\n",
    "train_batch_len = len(train_loader)\n",
    "train_batch_len\n",
    "test_batch_len = len(test_loader)\n",
    "test_batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_CV6HHIyvwJ"
   },
   "outputs": [],
   "source": [
    "category_dict = {\n",
    "    'airplane': 0, \n",
    "    'automobile': 1, \n",
    "    'bird': 2, \n",
    "    'cat': 3, \n",
    "    'deer': 4, \n",
    "    'dog': 5, \n",
    "    'frog': 6, \n",
    "    'horse': 7, \n",
    "    'ship': 8, \n",
    "    'truck': 9\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xdE57azyvwM"
   },
   "source": [
    "---\n",
    "---\n",
    "## Common Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixup \n",
    "From [Facebook AI Research](https://github.com/facebookresearch/mixup-cifar10/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if device == 'cuda':\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ZQmGXmvyvwM"
   },
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_correct_rate = []\n",
    "train_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qobPAnZVyvwN"
   },
   "outputs": [],
   "source": [
    "def training(epoch_num: int, model_name: str, net):    \n",
    "    \n",
    "    final_loss = 0.0\n",
    "    \n",
    "    total = train_size\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    for epoch in range(epoch_num):  # loop over the dataset multiple times\n",
    "        \n",
    "        correct = 0\n",
    "        run_loss  = 0.0\n",
    "        \n",
    "        train_bar = progress_bar(train_batch_len)\n",
    "        \n",
    "        print_time_and_msg(f\"Training for epoch {epoch + 1}\")\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader, 0):\n",
    "                        \n",
    "            if device == 'cuda':\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "            # Mixup\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, 1.0)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            run_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            final_loss = round(run_loss / train_batch_len, 3)\n",
    "            \n",
    "            train_bar.update(batch_idx)\n",
    "        \n",
    "        correct_rate = correct / total\n",
    "        if epoch == 0:\n",
    "            save_model(model_name, net)\n",
    "        elif correct_rate > max(train_correct_rate):\n",
    "            save_model(model_name, net)\n",
    "        elif (epoch + 1) % 100 == 0:\n",
    "            save_model(model_name, net)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        print_time_and_msg(f\"Trained Epoch {epoch + 1}. loss: {final_loss}. Correct/Total: {correct}/{total}, Correct rate: {correct_rate}\")\n",
    "        train_correct_rate.append(correct_rate)\n",
    "        train_loss.append(final_loss)\n",
    "        print()\n",
    "    \n",
    "    print_time_and_msg(f'Finished training total {train_size} data for {epoch_num} epoches,\\nthe final loss is {final_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d9e2HRpQyvwP"
   },
   "source": [
    "### Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NUm3wxi-yvwQ"
   },
   "outputs": [],
   "source": [
    "def testing(net):\n",
    "    test_bar = progress_bar(test_batch_len)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ti, (inputs, targets) in enumerate(test_loader):\n",
    "            \n",
    "            if device == 'cuda':\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            test_bar.update(ti)\n",
    "    print()\n",
    "    print(f'Accuracy of the network on the {test_size} test images: {100 * correct / total}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1hxNc6byvwS"
   },
   "source": [
    "### Test for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6q6HaiKNyvwS"
   },
   "outputs": [],
   "source": [
    "def test_for_each(net):\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    \n",
    "    test_bar = progress_bar(test_batch_len)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tei, (inputs, targets) in enumerate(test_loader):\n",
    "            \n",
    "            if device == 'cuda':\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == targets).squeeze()\n",
    "            for i in range(4):\n",
    "                label = targets[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "            \n",
    "            test_bar.update(tei)\n",
    "    \n",
    "    print()\n",
    "    for i in range(10):\n",
    "        print(f'Accuracy of {list(category_dict)[i]} : {round(100 * class_correct[i] / class_total[i], 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q3gtWMo7yvwU"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "On_3Qd1tyvwV"
   },
   "outputs": [],
   "source": [
    "def save_model(model_name: str, net):\n",
    "    PATH_basic = f'{gd_root}/models/{model_name}'\n",
    "    torch.save(net.state_dict(), PATH_basic)\n",
    "    print_time_and_msg(f\"Saved model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_V_DDVByvwY"
   },
   "source": [
    "### Print Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ymsb7YcJyvwZ"
   },
   "outputs": [],
   "source": [
    "def print_time_and_msg(msg: str) -> str:\n",
    "    now_time = time.time()\n",
    "    readable_time = datetime.fromtimestamp(now_time, pytz.timezone('Asia/Shanghai')).strftime(f'[%H:%M:%S] - {msg}')\n",
    "    print(readable_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xv6iNj0Iyvwc"
   },
   "source": [
    "### Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qFC1tPQyvwd"
   },
   "outputs": [],
   "source": [
    "def progress_bar(max_val: int):\n",
    "    bar = ProgressBar(\n",
    "        widgets=[\n",
    "            'Progress: ',\n",
    "            Percentage(), ' ', \n",
    "            Bar('#'), ' ', \n",
    "            Timer(), ' ', \n",
    "            ETA(), ' '\n",
    "        ], \n",
    "        max_value=max_val\n",
    "    )\n",
    "    return bar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_info():\n",
    "    plt.figure(figsize=(20,10))\n",
    "    x = [i for i in range(len(train_correct_rate))]\n",
    "    y1 = train_correct_rate\n",
    "    y2 = train_loss\n",
    "    \n",
    "    plt.plot(x, y1, label=\"train correct rate\")\n",
    "    plt.plot(x, y2, label=\"train loss\")\n",
    "    \n",
    "    plt.legend(loc=0, numpoints=1)\n",
    "    \n",
    "    plt.xlabel(\"Epoch\")\n",
    "    \n",
    "# plot_train_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uh2PBy0Myvwf"
   },
   "source": [
    "### Unpickle CIFAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1489,
     "status": "ok",
     "timestamp": 1576304904840,
     "user": {
      "displayName": "Tai To CHAN",
      "photoUrl": "",
      "userId": "16856483288969111373"
     },
     "user_tz": -480
    },
    "id": "6uz2DilZyvwg",
    "outputId": "484838c1-8b05-475d-e8fd-c0c047dc9950"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'num_cases_per_batch': 10000,\n",
       " b'label_names': [b'airplane',\n",
       "  b'automobile',\n",
       "  b'bird',\n",
       "  b'cat',\n",
       "  b'deer',\n",
       "  b'dog',\n",
       "  b'frog',\n",
       "  b'horse',\n",
       "  b'ship',\n",
       "  b'truck'],\n",
       " b'num_vis': 3072}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unpickle_cifar10(file: str) -> dict:\n",
    "    full_name = f\"{gd_root}/data/cifar-10-batches-py/{file}\"\n",
    "    \n",
    "    with open(full_name, 'rb') as fi:\n",
    "        cifar10_dict = pickle.load(fi, encoding='bytes')\n",
    "    return cifar10_dict\n",
    "\n",
    "cifar_meta = unpickle_cifar10(\"batches.meta\")\n",
    "cifar_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat([out,x], 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(F.relu(self.bn(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        num_planes = 2*growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
    "        num_planes += nblocks[0]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans1 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
    "        num_planes += nblocks[3]*growth_rate\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "        self.linear = nn.Linear(num_planes, num_classes)\n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def DenseNet121():\n",
    "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DenaseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FsQV6evnyvxc"
   },
   "source": [
    "#### Go to [training function](#Training-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_net = DenseNet121()\n",
    "densenet_net.to(device)\n",
    "if device == 'cuda':\n",
    "    densenet_net = torch.nn.DataParallel(densenet_net)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(densenet_net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:28:38] - Loaded model: DenseNet.pth\n"
     ]
    }
   ],
   "source": [
    "model_PATH = f\"{gd_root}/models/{PATH_Dense}\"\n",
    "densenet_net.load_state_dict(torch.load(model_PATH))\n",
    "print_time_and_msg(f\"Loaded model: {PATH_Dense}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:33:47] - Start training Dense Net for 100 epoches\n",
      "\n",
      "[20:33:47] - Training for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:06 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:34:54] - Saved model\n",
      "[20:34:54] - Trained Epoch 1. loss: 0.821. Correct/Total: 27201/50000, Correct rate: 0.54402\n",
      "\n",
      "[20:34:54] - Training for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:06 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:36:01] - Saved model\n",
      "[20:36:01] - Trained Epoch 2. loss: 0.817. Correct/Total: 27364/50000, Correct rate: 0.54728\n",
      "\n",
      "[20:36:01] - Training for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:04 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:37:06] - Saved model\n",
      "[20:37:06] - Trained Epoch 3. loss: 0.8. Correct/Total: 27964/50000, Correct rate: 0.55928\n",
      "\n",
      "[20:37:06] - Training for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:06 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:38:13] - Trained Epoch 4. loss: 0.796. Correct/Total: 26625/50000, Correct rate: 0.5325\n",
      "\n",
      "[20:38:13] - Training for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:04 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:39:19] - Trained Epoch 5. loss: 0.779. Correct/Total: 26549/50000, Correct rate: 0.53098\n",
      "\n",
      "[20:39:19] - Training for epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:06 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:40:25] - Trained Epoch 6. loss: 0.77. Correct/Total: 25678/50000, Correct rate: 0.51356\n",
      "\n",
      "[20:40:25] - Training for epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:04 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:41:30] - Trained Epoch 7. loss: 0.789. Correct/Total: 27128/50000, Correct rate: 0.54256\n",
      "\n",
      "[20:41:30] - Training for epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:06 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:42:37] - Trained Epoch 8. loss: 0.779. Correct/Total: 27417/50000, Correct rate: 0.54834\n",
      "\n",
      "[20:42:37] - Training for epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:05 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:43:43] - Trained Epoch 9. loss: 0.772. Correct/Total: 26539/50000, Correct rate: 0.53078\n",
      "\n",
      "[20:43:43] - Training for epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:05 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:44:50] - Trained Epoch 10. loss: 0.751. Correct/Total: 26402/50000, Correct rate: 0.52804\n",
      "\n",
      "[20:44:50] - Training for epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:05 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:45:55] - Trained Epoch 11. loss: 0.778. Correct/Total: 26917/50000, Correct rate: 0.53834\n",
      "\n",
      "[20:45:55] - Training for epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:05 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:47:02] - Trained Epoch 12. loss: 0.781. Correct/Total: 27618/50000, Correct rate: 0.55236\n",
      "\n",
      "[20:47:02] - Training for epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:04 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:48:07] - Saved model\n",
      "[20:48:07] - Trained Epoch 13. loss: 0.751. Correct/Total: 28184/50000, Correct rate: 0.56368\n",
      "\n",
      "[20:48:07] - Training for epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:05 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:49:13] - Trained Epoch 14. loss: 0.773. Correct/Total: 26310/50000, Correct rate: 0.5262\n",
      "\n",
      "[20:49:13] - Training for epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:04 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:50:18] - Trained Epoch 15. loss: 0.759. Correct/Total: 26216/50000, Correct rate: 0.52432\n",
      "\n",
      "[20:50:18] - Training for epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:05 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:24] - Trained Epoch 16. loss: 0.75. Correct/Total: 28031/50000, Correct rate: 0.56062\n",
      "\n",
      "[20:51:24] - Training for epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:05 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:52:31] - Trained Epoch 17. loss: 0.778. Correct/Total: 25362/50000, Correct rate: 0.50724\n",
      "\n",
      "[20:52:31] - Training for epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:05 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:53:36] - Trained Epoch 18. loss: 0.75. Correct/Total: 27513/50000, Correct rate: 0.55026\n",
      "\n",
      "[20:53:36] - Training for epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:05 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:54:43] - Trained Epoch 19. loss: 0.756. Correct/Total: 27117/50000, Correct rate: 0.54234\n",
      "\n",
      "[20:54:43] - Training for epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  30% |#######                 | Elapsed Time: 0:00:19 ETA:   0:00:45 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-c011d27d66ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint_time_and_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Start training Dense Net for {epoches} epoches\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPATH_Dense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdensenet_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-5c175ca2d9b7>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(epoch_num, model_name, net)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mrun_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                     \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0mparam_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_time_and_msg(f\"Start training Dense Net for {epoches} epoches\\n\")\n",
    "training(epoch_num=epoches, model_name=PATH_Dense, net=densenet_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PATH = f\"{gd_root}/models/{PATH_Dense}\"\n",
    "test_densenet_net = DenseNet121()\n",
    "test_densenet_net.to(device)\n",
    "if device == 'cuda':\n",
    "    test_densenet_net = torch.nn.DataParallel(test_densenet_net)\n",
    "    cudnn.benchmark = True\n",
    "test_densenet_net.load_state_dict(torch.load(model_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  82% |###################     | Elapsed Time: 0:00:08 ETA:   0:00:01 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of the network on the 10000 test images: 94.94%\n"
     ]
    }
   ],
   "source": [
    "testing(test_densenet_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  82% |###################     | Elapsed Time: 0:00:09 ETA:   0:00:02 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of airplane : 95.58%\n",
      "Accuracy of automobile : 98.26%\n",
      "Accuracy of bird : 90.79%\n",
      "Accuracy of cat : 89.15%\n",
      "Accuracy of deer : 93.18%\n",
      "Accuracy of dog : 93.81%\n",
      "Accuracy of frog : 97.52%\n",
      "Accuracy of horse : 98.44%\n",
      "Accuracy of ship : 97.35%\n",
      "Accuracy of truck : 97.06%\n"
     ]
    }
   ],
   "source": [
    "test_for_each(test_densenet_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "77WiD6JXyvxt"
   },
   "source": [
    "---\n",
    "---\n",
    "## Run the model in project test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJALJcIxyvxw"
   },
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KzaLs-wGyvxx",
    "outputId": "8f3dc205-5a92-4010-a1a0-72c07cd72070"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 3, 32, 32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_testset = np.load(f\"{gd_root}/data/test_data/y_test.npy\").astype(np.uint8)\n",
    "project_testset = np.moveaxis(project_testset, -1, 2)\n",
    "project_testset = np.moveaxis(project_testset, 2, 1)\n",
    "project_testset.shape\n",
    "type(project_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2dOcu_QQyvxz"
   },
   "source": [
    "### Convert to DataLoader ([Convertion Reference](https://stackoverflow.com/questions/44429199/how-to-load-a-list-of-numpy-arrays-to-pytorch-dataset-loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEePOqX9yvx0"
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "tensor_testset = torch.Tensor(project_testset)\n",
    "the_dataset = data.TensorDataset(tensor_testset)\n",
    "project_test_loader = data.DataLoader(the_dataset, \n",
    "                                      batch_size=32,\n",
    "                                      num_workers=8,\n",
    "                                      pin_memory=True\n",
    "                                     )\n",
    "test_batch_len = len(project_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vXhVXVBfyvx7"
   },
   "source": [
    "### Run the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "92iuLfst-aKE"
   },
   "outputs": [],
   "source": [
    "predict_result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PgeSzkXSyvx8"
   },
   "outputs": [],
   "source": [
    "def run_project_test(net, model_path):\n",
    "    net.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    test_pbar = progress_bar(test_batch_len)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for index, imgs in enumerate(project_test_loader):\n",
    "            \n",
    "            imgs = torch.stack([torch.Tensor(i) for i in imgs]).squeeze()\n",
    "            \n",
    "            if device == 'cuda':\n",
    "                imgs.cuda()\n",
    "                \n",
    "            outputs = net(imgs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            for p in predicted:\n",
    "                predict_result.append(p.item())\n",
    "            \n",
    "            test_pbar.update(index)\n",
    "      \n",
    "    print_time_and_msg(f\"Testd {len(predict_result)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qkO7pEtGyvyA"
   },
   "outputs": [],
   "source": [
    "model_PATH = f\"{gd_root}/models/{PATH_Dense}\"  # PATH_DPN\n",
    "# p_test_net = DPN92()\n",
    "p_test_net = DenseNet121()\n",
    "if device == 'cuda':\n",
    "    p_test_net.to(device)\n",
    "    p_test_net = torch.nn.DataParallel(p_test_net)\n",
    "    cudnn.benchmark = True\n",
    "run_project_test(p_test_net, model_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5E7gyd8yvyD"
   },
   "source": [
    "### Save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wA2cHmKyyvyE"
   },
   "outputs": [],
   "source": [
    "def save_to_csv(result: list):\n",
    "    result_df = pd.DataFrame(\n",
    "        list(predict_result),\n",
    "        columns=[\"Category\"]\n",
    "    )\n",
    "    result_df.index.name = \"Index\"\n",
    "    print(result_df.shape)\n",
    "    result_df.to_csv(f\"{gd_root}/predict_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcf6hjlWyvyI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 1)\n"
     ]
    }
   ],
   "source": [
    "save_to_csv(predict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Elytc4GGyvyL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1Tu2bqE5yvwk",
    "S15lvkBqyvwz",
    "B81yKDDJyvw2",
    "h0W9w1Dfyvw7",
    "orrJg-sjyvw-",
    "CqggjSvSyvw_",
    "UguTeLXRyvxA",
    "Gcg_NYYeyvxE",
    "OudCsGPayvxH",
    "lGFo0oTzyvxK"
   ],
   "name": "Project_Image_classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
