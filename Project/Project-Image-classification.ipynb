{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5RtmpwqNyvvw"
   },
   "source": [
    "# CS4487 Project: Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CM4u04Ptyvvz"
   },
   "source": [
    "## Regarding course project\n",
    "* External training data is not allowed. Learning can only be performed on the 50,000 training images in **CIFAR10**\n",
    "* The [**CIFAR10** dataset](https://www.cs.toronto.edu/~kriz/cifar.html) was comprised of 60,000 32 Ã— 32 color photographs of objects from 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images.\n",
    "* Submit the results at [Kaggle CS4487 Course Project](https://www.kaggle.com/c/cs4487cp/)\n",
    "* The second test set containing roughly 2,000 images (refer to test_data/y_test.npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KC9dhUb2yvvz"
   },
   "source": [
    "## Evaluation Criteria \n",
    "\n",
    "* ### Classification Accuaracy:\n",
    "  ### $$ Acc(f, D) = \\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{I}[y^i = f(x^i)]$$\n",
    "\n",
    "* ### File Format:\n",
    "Upload a file in CSV format, such as:\n",
    "\n",
    "|         **Index**         |         **Category**        |\n",
    "|---------------------------|-----------------------------|\n",
    "| 0                         | 8                           |\n",
    "| 1                         | 4                           |\n",
    "| 2                         | 6                           |\n",
    "| 3                         | 1                           |\n",
    "| ......                    | ......                      |\n",
    "| 11999                     | 2                           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlAURl9gyvv1"
   },
   "source": [
    "PyTorch Tutorial: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hEdlTSHSyvv3"
   },
   "source": [
    "---\n",
    "## Table of content (for quick jump)\n",
    "* ### [Import Library](#Import-Library)\n",
    "* ### [Data Preparation](#Data-Preparation)\n",
    "* ### [Common Functions](#Common-Functions)\n",
    "  * #### [Training Function](#Training-function)\n",
    "  * #### [Testing Function](#Testing-function)\n",
    "  * #### [Test for each category](#Test-for-each-category)\n",
    "  * #### [Save Model](#Save-Model)\n",
    "  * #### [Print Time](#Print-Time)\n",
    "  * #### [Progress Bar](#Progress-Bar)\n",
    "* ### [Dual Path Networks (DPN)](#Dual-Path-Networks)\n",
    "  * #### [Training](#Train-DPN)\n",
    "  * #### [Testing](#Test-DPN)\n",
    "* ### [DenseNet (DN)](#DenseNet)\n",
    "  * #### [Training]()\n",
    "  * #### [Testing]()\n",
    "* ### [Run the model in project test data](#Run-the-model-in-project-test-data)\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gm8PdbZpyvv4"
   },
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install progressbar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5IH1vwTyvv5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "from _datetime import datetime\n",
    "import pytz\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from progressbar import *\n",
    "from sys import platform\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_root = \".\"  # For loacal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == \"win32\":\n",
    "    gd_root = \".\"  # For loacal run\n",
    "    \n",
    "else:  # run on Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    gd_root = \"gdrive/My Drive/Colab Notebooks/CS4487/Project\"\n",
    "\n",
    "gd_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1A24rM-yvv8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "basic_path = \"basic_cifar_net.pth\"\n",
    "PATH_DPN = \"DPN_cifar.pth\"\n",
    "PATH_Dense = \"DenseNet.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RBvQrtukyvwA"
   },
   "source": [
    "---\n",
    "---\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8Ar3LQxyvwB"
   },
   "source": [
    "### Read CIFAR10 data\n",
    "* The archive contains the files data_batch_1, data_batch_2, ..., data_batch_5, as well as test_batch. Each of these files is a Python \"pickled\" object produced with cPickle.\n",
    "* Loaded in this way, each of the batch files contains a dictionary with the following elements:\n",
    "  * **data**: \n",
    "    * a 10,000 x 3,072 numpy array of uint8s. Each row of the array stores a 32 x 32 colour image. \n",
    "    * The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. \n",
    "    * The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
    "  * **labels**: \n",
    "    * a list of 10,000 numbers in the range 0 - 9. \n",
    "    * The number at index i indicates the label of the ith image in the array data.\n",
    "* The dataset contains another file, called batches.meta. It too contains a Python dictionary object. It has the following entries:\n",
    "  * **label_names** -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. \n",
    "  * For example, label_names[0] == \"airplane\", label_names[1] == \"automobile\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37XbAGx7yvwB"
   },
   "source": [
    "### Loading and normalizing CIFAR10 to dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oyQkgqNOyvwC"
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DB2gBIfPyvwE"
   },
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root=f'{gd_root}/data', train=True, download=False, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, \n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True, \n",
    "                                           num_workers=8, \n",
    "                                           pin_memory=True\n",
    "                                          )\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=f'{gd_root}/data', train=False, download=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, \n",
    "                                          batch_size=32,\n",
    "                                          shuffle=False, \n",
    "                                          num_workers=8,\n",
    "                                          pin_memory=True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3289,
     "status": "ok",
     "timestamp": 1576304876813,
     "user": {
      "displayName": "Tai To CHAN",
      "photoUrl": "",
      "userId": "16856483288969111373"
     },
     "user_tz": -480
    },
    "id": "iZX-q8HpyvwH",
    "outputId": "8344fa58-1354-40ea-f53b-b4c8217ca5eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(trainset)\n",
    "train_size\n",
    "test_size = len(testset)\n",
    "test_size\n",
    "# train_batch_len = len(train_loader)\n",
    "# train_batch_len\n",
    "# test_batch_len = len(test_loader)\n",
    "# test_batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_CV6HHIyvwJ"
   },
   "outputs": [],
   "source": [
    "category_dict = {\n",
    "    'airplane': 1, \n",
    "    'automobile': 2, \n",
    "    'bird': 3, \n",
    "    'cat': 4, \n",
    "    'deer': 5, \n",
    "    'dog': 6, \n",
    "    'frog': 7, \n",
    "    'horse': 8, \n",
    "    'ship': 9, \n",
    "    'truck': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xdE57azyvwM"
   },
   "source": [
    "---\n",
    "---\n",
    "## Common Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixup \n",
    "From [Facebook AI Research](https://github.com/facebookresearch/mixup-cifar10/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if device == 'cuda':\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ZQmGXmvyvwM"
   },
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_correct_rate = []\n",
    "train_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qobPAnZVyvwN"
   },
   "outputs": [],
   "source": [
    "def training(epoch_num: int, model_name: str, net):    \n",
    "    \n",
    "    final_loss = 0.0\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for epoch in range(epoch_num):  # loop over the dataset multiple times\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        train_bar = progress_bar(train_batch_len)\n",
    "        \n",
    "        print_time_and_msg(f\"Training for epoch {epoch + 1}\")\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader, 0):\n",
    "                        \n",
    "            if device == 'cuda':\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "            # Mixup\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, 1.0)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            final_loss = round(running_loss / train_batch_len, 3)\n",
    "            \n",
    "            train_bar.update(batch_idx)\n",
    "        \n",
    "        correct_rate = correct / total\n",
    "        if epoch == 0:\n",
    "            save_model(model_name, net)\n",
    "        elif correct_rate > train_correct_rate[-1]:\n",
    "            save_model(model_name, net)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        print_time_and_msg(f\"Trained Epoch {epoch + 1}. loss: {final_loss}. Correct/Total: {correct}/{total}, Correct rate: {correct_rate}\")\n",
    "        train_correct_rate.append(correct_rate)\n",
    "        train_loss.append(final_loss)\n",
    "        print()\n",
    "    \n",
    "    print_time_and_msg(f'Finished training total {train_size} data for {epoch_num} epoches,\\nthe final loss is {final_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d9e2HRpQyvwP"
   },
   "source": [
    "### Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NUm3wxi-yvwQ"
   },
   "outputs": [],
   "source": [
    "def testing(net):\n",
    "    test_bar = progress_bar(test_batch_len)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ti, (inputs, targets) in enumerate(test_loader):\n",
    "            \n",
    "            if device == 'cuda':\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            test_bar.update(ti)\n",
    "    print()\n",
    "    print(f'Accuracy of the network on the {len(test_loader)} test images: {100 * correct / total}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1hxNc6byvwS"
   },
   "source": [
    "### Test for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6q6HaiKNyvwS"
   },
   "outputs": [],
   "source": [
    "def test_for_each(net):\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        for (inputs, targets) in test_loader:\n",
    "            \n",
    "            if device == 'cuda':\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == targets).squeeze()\n",
    "            for i in range(4):\n",
    "                label = targets[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    print()\n",
    "    for i in range(10):\n",
    "        print(f'Accuracy of {list(category_dict)[i]} : {round(100 * class_correct[i] / class_total[i], 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q3gtWMo7yvwU"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "On_3Qd1tyvwV"
   },
   "outputs": [],
   "source": [
    "def save_model(model_name: str, net):\n",
    "    PATH_basic = f'{gd_root}/models/{model_name}'\n",
    "    torch.save(net.state_dict(), PATH_basic)\n",
    "    print_time_and_msg(f\"Saved model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_V_DDVByvwY"
   },
   "source": [
    "### Print Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ymsb7YcJyvwZ"
   },
   "outputs": [],
   "source": [
    "def print_time_and_msg(msg: str) -> str:\n",
    "    now_time = time.time()\n",
    "    readable_time = datetime.fromtimestamp(now_time, pytz.timezone('Asia/Shanghai')).strftime(f'[%H:%M:%S] - {msg}')\n",
    "    print(readable_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xv6iNj0Iyvwc"
   },
   "source": [
    "### Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qFC1tPQyvwd"
   },
   "outputs": [],
   "source": [
    "def progress_bar(max_val: int):\n",
    "    bar = ProgressBar(\n",
    "        widgets=[\n",
    "            'Progress: ',\n",
    "            Percentage(), ' ', \n",
    "            Bar('#'), ' ', \n",
    "            Timer(), ' ', \n",
    "            ETA(), ' '\n",
    "        ], \n",
    "        max_value=max_val\n",
    "    )\n",
    "    return bar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_info(data: list, label: str):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    x = [i for i in range(len(data))]\n",
    "    y = data\n",
    "    plt.plot(x, y, label=label)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(np.arange(min(x), max(x)+1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uh2PBy0Myvwf"
   },
   "source": [
    "### Unpickle CIFAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1489,
     "status": "ok",
     "timestamp": 1576304904840,
     "user": {
      "displayName": "Tai To CHAN",
      "photoUrl": "",
      "userId": "16856483288969111373"
     },
     "user_tz": -480
    },
    "id": "6uz2DilZyvwg",
    "outputId": "484838c1-8b05-475d-e8fd-c0c047dc9950"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'num_cases_per_batch': 10000,\n",
       " b'label_names': [b'airplane',\n",
       "  b'automobile',\n",
       "  b'bird',\n",
       "  b'cat',\n",
       "  b'deer',\n",
       "  b'dog',\n",
       "  b'frog',\n",
       "  b'horse',\n",
       "  b'ship',\n",
       "  b'truck'],\n",
       " b'num_vis': 3072}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unpickle_cifar10(file: str) -> dict:\n",
    "    full_name = f\"{gd_root}/data/cifar-10-batches-py/{file}\"\n",
    "    \n",
    "    with open(full_name, 'rb') as fi:\n",
    "        cifar10_dict = pickle.load(fi, encoding='bytes')\n",
    "    return cifar10_dict\n",
    "\n",
    "cifar_meta = unpickle_cifar10(\"batches.meta\")\n",
    "cifar_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HDJjhugXyvxO"
   },
   "source": [
    "## Dual Path Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UlU4Y5myvxO"
   },
   "source": [
    "### Define DPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jwei3rzsyvxP"
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, last_planes, in_planes, out_planes, dense_depth, stride, first_layer):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.out_planes = out_planes\n",
    "        self.dense_depth = dense_depth\n",
    "\n",
    "        self.conv1 = nn.Conv2d(last_planes, in_planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=32, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv3 = nn.Conv2d(in_planes, out_planes + dense_depth, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_planes+dense_depth)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if first_layer:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(last_planes, out_planes + dense_depth, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_planes + dense_depth)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        x = self.shortcut(x)\n",
    "        d = self.out_planes\n",
    "        out = torch.cat([x[:,:d,:,:] + out[:,:d,:,:], x[:,d:,:,:], out[:,d:,:,:]], 1)\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRALbnL1yvxR"
   },
   "outputs": [],
   "source": [
    "class DPN(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(DPN, self).__init__()\n",
    "        in_planes, out_planes = cfg['in_planes'], cfg['out_planes']\n",
    "        num_blocks, dense_depth = cfg['num_blocks'], cfg['dense_depth']\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.last_planes = 64\n",
    "        self.layer1 = self._make_layer(in_planes[0], out_planes[0], num_blocks[0], dense_depth[0], stride=1)\n",
    "        self.layer2 = self._make_layer(in_planes[1], out_planes[1], num_blocks[1], dense_depth[1], stride=2)\n",
    "        self.layer3 = self._make_layer(in_planes[2], out_planes[2], num_blocks[2], dense_depth[2], stride=2)\n",
    "        self.layer4 = self._make_layer(in_planes[3], out_planes[3], num_blocks[3], dense_depth[3], stride=2)\n",
    "        self.linear = nn.Linear(out_planes[3] + (num_blocks[3] + 1) * dense_depth[3], 10)\n",
    "\n",
    "    def _make_layer(self, in_planes, out_planes, num_blocks, dense_depth, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for i,stride in enumerate(strides):\n",
    "            layers.append(Bottleneck(self.last_planes, in_planes, out_planes, dense_depth, stride, i==0))\n",
    "            self.last_planes = out_planes + (i + 2) * dense_depth\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVzlzN19yvxU"
   },
   "outputs": [],
   "source": [
    "def DPN26():\n",
    "    cfg = {\n",
    "        'in_planes': (96,192,384,768),\n",
    "        'out_planes': (256,512,1024,2048),\n",
    "        'num_blocks': (2,2,2,2),\n",
    "        'dense_depth': (16,32,24,128)\n",
    "    }\n",
    "    return DPN(cfg)\n",
    "\n",
    "def DPN92():\n",
    "    cfg = {\n",
    "        'in_planes': (96,192,384,768),\n",
    "        'out_planes': (256,512,1024,2048),\n",
    "        'num_blocks': (3,4,20,3),\n",
    "        'dense_depth': (16,32,24,128)\n",
    "    }\n",
    "    return DPN(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klwLrEFMyvxW"
   },
   "source": [
    "### Train DPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6512,
     "status": "ok",
     "timestamp": 1576304944308,
     "user": {
      "displayName": "Tai To CHAN",
      "photoUrl": "",
      "userId": "16856483288969111373"
     },
     "user_tz": -480
    },
    "id": "JikSvrm8yvxW",
    "outputId": "3655a091-8160-4789-ed14-be030c54e070"
   },
   "outputs": [],
   "source": [
    "DPN_net = DPN92()\n",
    "DPN_net.to(device)\n",
    "if device == 'cuda':\n",
    "    DPN_net = torch.nn.DataParallel(DPN_net)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgrLCydtyvxZ"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(DPN_net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FsQV6evnyvxc"
   },
   "source": [
    "#### Go to [training function](#Training-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 956,
     "status": "ok",
     "timestamp": 1576305007944,
     "user": {
      "displayName": "Tai To CHAN",
      "photoUrl": "",
      "userId": "16856483288969111373"
     },
     "user_tz": -480
    },
    "id": "GsuU7Pklerei",
    "outputId": "0dc6ac9d-b7b7-4e37-8cb6-604449780dc2"
   },
   "outputs": [],
   "source": [
    "model_PATH = f\"{gd_root}/models/{PATH_DPN}\"\n",
    "DPN_net.load_state_dict(torch.load(model_PATH))\n",
    "print_time_and_msg(f\"Loaded model: {PATH_DPN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 969
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11614309,
     "status": "error",
     "timestamp": 1576316660920,
     "user": {
      "displayName": "Tai To CHAN",
      "photoUrl": "",
      "userId": "16856483288969111373"
     },
     "user_tz": -480
    },
    "id": "8e6JCJy4yvxc",
    "outputId": "8c764e78-25ff-42ab-fa52-95794b36d1fe"
   },
   "outputs": [],
   "source": [
    "print_time_and_msg(f\"Start training for {epoches} epoches\\n\")\n",
    "training(epoch_num=epoches, model_name=PATH_DPN, net=DPN_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_info(train_correct_rate, \"train correct rate\")\n",
    "plot_train_info(train_loss, \"train loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ZIZtCKlyvxe"
   },
   "source": [
    "### Test DPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1760,
     "status": "ok",
     "timestamp": 1576304959213,
     "user": {
      "displayName": "Tai To CHAN",
      "photoUrl": "",
      "userId": "16856483288969111373"
     },
     "user_tz": -480
    },
    "id": "ls91n_GDyvxf",
    "outputId": "1d94f7e7-ddb6-4ad9-cc8e-0561e7918c58"
   },
   "outputs": [],
   "source": [
    "model_PATH = f\"{gd_root}/models/{PATH_DPN}\"\n",
    "test_net = DPN92()\n",
    "if device == 'cuda':\n",
    "    test_net = torch.nn.DataParallel(test_net)\n",
    "    cudnn.benchmark = True\n",
    "test_net.load_state_dict(torch.load(model_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24583,
     "status": "error",
     "timestamp": 1576304986287,
     "user": {
      "displayName": "Tai To CHAN",
      "photoUrl": "",
      "userId": "16856483288969111373"
     },
     "user_tz": -480
    },
    "id": "QvEIBZnbyvxg",
    "outputId": "944788eb-ce52-43d4-fd78-99cd33eaafb0"
   },
   "outputs": [],
   "source": [
    "testing(test_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1OATaiLmyvxi"
   },
   "outputs": [],
   "source": [
    "test_for_each(test_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat([out,x], 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(F.relu(self.bn(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        num_planes = 2*growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
    "        num_planes += nblocks[0]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans1 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
    "        num_planes += nblocks[3]*growth_rate\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "        self.linear = nn.Linear(num_planes, num_classes)\n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def DenseNet121():\n",
    "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DenaseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (dense1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (bn1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (trans1): Transition(\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (dense2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (bn1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (bn1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (bn1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (bn1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (trans2): Transition(\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (dense3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (bn1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (bn1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (bn1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (bn1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (bn1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (bn1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (bn1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (bn1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (bn1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (bn1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (bn1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (bn1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (23): Bottleneck(\n",
       "      (bn1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (trans3): Transition(\n",
       "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (dense4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (bn1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (bn1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (bn1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (bn1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (bn1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (bn1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (bn1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (bn1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (bn1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (bn1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "densenet_net = DenseNet121()\n",
    "densenet_net.to(device)\n",
    "if device == 'cuda':\n",
    "    densenet_net = torch.nn.DataParallel(densenet_net)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(densenet_net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PATH = f\"{gd_root}/models/{PATH_Dense}\"\n",
    "densenet_net.load_state_dict(torch.load(model_PATH))\n",
    "print_time_and_msg(f\"Loaded model: {PATH_Dense}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:20:06] - Start training Densenet Net for 500 epoches\n",
      "\n",
      "[18:20:06] - Training for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:06 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:21:15] - Saved model\n",
      "[18:21:15] - Trained Epoch 1. loss: 1.829. Correct/Total: 14226/50000, Correct rate: 0.28452\n",
      "\n",
      "[18:21:15] - Training for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:04 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:22:19] - Saved model\n",
      "[18:22:19] - Trained Epoch 2. loss: 1.538. Correct/Total: 32873/100000, Correct rate: 0.32873\n",
      "\n",
      "[18:22:19] - Training for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  99% |####################### | Elapsed Time: 0:01:04 ETA:   0:00:00 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:23:25] - Saved model\n",
      "[18:23:25] - Trained Epoch 3. loss: 1.402. Correct/Total: 54017/150000, Correct rate: 0.36011333333333334\n",
      "\n",
      "[18:23:25] - Training for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  71% |#################       | Elapsed Time: 0:00:46 ETA:   0:00:18 "
     ]
    }
   ],
   "source": [
    "print_time_and_msg(f\"Start training Densenet Net for {epoches} epoches\\n\")\n",
    "training(epoch_num=epoches, model_name=PATH_Dense, net=densenet_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PATH = f\"{gd_root}/models/{PATH_Dense}\"\n",
    "test_densenet_net = DenseNet121()\n",
    "test_densenet_net.to(device)\n",
    "if device == 'cuda':\n",
    "    test_densenet_net = torch.nn.DataParallel(test_densenet_net)\n",
    "    cudnn.benchmark = True\n",
    "test_densenet_net.load_state_dict(torch.load(model_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "77WiD6JXyvxt"
   },
   "source": [
    "---\n",
    "---\n",
    "## Run the model in project test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJALJcIxyvxw"
   },
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KzaLs-wGyvxx",
    "outputId": "8f3dc205-5a92-4010-a1a0-72c07cd72070"
   },
   "outputs": [],
   "source": [
    "project_testset = np.load(f\"{gd_root}/data/test_data/y_test.npy\").astype(np.uint8)\n",
    "project_testset = np.moveaxis(project_testset, -1, 2)\n",
    "project_testset = np.moveaxis(project_testset, 2, 1)\n",
    "project_testset.shape\n",
    "type(project_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2dOcu_QQyvxz"
   },
   "source": [
    "### Convert to DataLoader ([Convertion Reference](https://stackoverflow.com/questions/44429199/how-to-load-a-list-of-numpy-arrays-to-pytorch-dataset-loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEePOqX9yvx0"
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "tensor_testset = torch.Tensor(project_testset)\n",
    "the_dataset = data.TensorDataset(tensor_testset)\n",
    "project_test_loader = data.DataLoader(the_dataset, \n",
    "                                      batch_size=32,\n",
    "                                      num_workers=8,\n",
    "                                      pin_memory=True\n",
    "                                     )\n",
    "test_batch_len = len(project_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vXhVXVBfyvx7"
   },
   "source": [
    "### Run the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "92iuLfst-aKE"
   },
   "outputs": [],
   "source": [
    "predict_result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PgeSzkXSyvx8"
   },
   "outputs": [],
   "source": [
    "def run_project_test(net, model_path):\n",
    "    net.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    test_pbar = progress_bar(test_batch_len)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for index, imgs in enumerate(project_test_loader):\n",
    "            \n",
    "            imgs = torch.stack([torch.Tensor(i) for i in imgs]).squeeze()\n",
    "            \n",
    "            if device == 'cuda':\n",
    "                imgs.cuda()\n",
    "                \n",
    "            outputs = net(imgs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            for p in predicted:\n",
    "                predict_result.append(p)\n",
    "            \n",
    "            test_pbar.update(index)\n",
    "      \n",
    "    print(len(predict_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qkO7pEtGyvyA"
   },
   "outputs": [],
   "source": [
    "model_PATH = f\"{gd_root}/models/{PATH_Dense}\"\n",
    "p_test_net = DenseNet121()\n",
    "if device == 'cuda':\n",
    "    p_test_net.to(device)\n",
    "    p_test_net = torch.nn.DataParallel(p_test_net)\n",
    "    cudnn.benchmark = True\n",
    "run_project_test(p_test_net, model_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Z7hl1ak-aKL",
    "outputId": "2cdfecd1-241e-49aa-a286-228b998286e6"
   },
   "outputs": [],
   "source": [
    "len(predict_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5E7gyd8yvyD"
   },
   "source": [
    "### Save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wA2cHmKyyvyE"
   },
   "outputs": [],
   "source": [
    "def save_to_csv(result: list):\n",
    "    result_df = pd.DataFrame(\n",
    "        list(predict_result),\n",
    "        columns=[\"Category\"]\n",
    "    )\n",
    "    result_df.index.name = \"Index\"\n",
    "    # result_df\n",
    "    result_df.to_csv(f\"{gd_root}/predict_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcf6hjlWyvyI"
   },
   "outputs": [],
   "source": [
    "save_to_csv(predict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Elytc4GGyvyL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1Tu2bqE5yvwk",
    "S15lvkBqyvwz",
    "B81yKDDJyvw2",
    "h0W9w1Dfyvw7",
    "orrJg-sjyvw-",
    "CqggjSvSyvw_",
    "UguTeLXRyvxA",
    "Gcg_NYYeyvxE",
    "OudCsGPayvxH",
    "lGFo0oTzyvxK"
   ],
   "name": "Project_Image_classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
